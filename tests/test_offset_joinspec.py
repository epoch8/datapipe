"""
Ð¢ÐµÑÑ‚ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ñ‡Ñ‚Ð¾ offset'Ñ‹ ÑÐ¾Ð·Ð´Ð°ÑŽÑ‚ÑÑ Ð´Ð»Ñ JoinSpec Ñ‚Ð°Ð±Ð»Ð¸Ñ† (Ñ join_keys).

Ð’Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ Ð±Ð°Ð³ Ð³Ð´Ðµ offset ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ð»ÑÑ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð³Ð»Ð°Ð²Ð½Ð¾Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ (posts),
Ð½Ð¾ Ð½Ðµ Ð´Ð»Ñ ÑÐ¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½Ð¾Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ (profiles) Ñ join_keys.
"""

import time

import pandas as pd
from sqlalchemy import Column, Integer, String

from datapipe.compute import ComputeInput
from datapipe.datatable import DataStore
from datapipe.step.batch_transform import BatchTransformStep
from datapipe.store.database import DBConn, TableStoreDB


def test_offset_created_for_joinspec_tables(dbconn: DBConn):
    """
    ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ñ‡Ñ‚Ð¾ offset ÑÐ¾Ð·Ð´Ð°ÐµÑ‚ÑÑ Ð´Ð»Ñ Ñ‚Ð°Ð±Ð»Ð¸Ñ† Ñ join_keys (JoinSpec).

    Ð¡Ñ†ÐµÐ½Ð°Ñ€Ð¸Ð¹:
    1. Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ posts Ð¸ profiles (profiles Ñ join_keys={'user_id': 'id'})
    2. Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ñ offset optimization
    3. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‡Ñ‚Ð¾ offset ÑÐ¾Ð·Ð´Ð°Ð½ Ð”Ð›Ð¯ ÐžÐ‘Ð•Ð˜Ð¥ Ñ‚Ð°Ð±Ð»Ð¸Ñ†: posts Ð˜ profiles
    """
    ds = DataStore(dbconn, create_meta_table=True)

    # 1. Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ posts Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ String Ð´Ð»Ñ id Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÑÐ¾Ð²Ð¿Ð°Ð´Ð°Ñ‚ÑŒ Ñ Ð¼ÐµÑ‚Ð°-Ñ‚Ð°Ð±Ð»Ð¸Ñ†ÐµÐ¹)
    posts_store = TableStoreDB(
        dbconn,
        "posts",
        [
            Column("id", String, primary_key=True),
            Column("user_id", String),
            Column("content", String),
        ],
        create_table=True,
    )
    posts = ds.create_table("posts", posts_store)

    # 2. Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ profiles Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ (ÑÐ¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½Ð¸Ðº)
    profiles_store = TableStoreDB(
        dbconn,
        "profiles",
        [Column("id", String, primary_key=True), Column("username", String)],
        create_table=True,
    )
    profiles = ds.create_table("profiles", profiles_store)

    # 3. Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ output Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ (id - primary key, Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ð¾Ðµ - Ð´Ð°Ð½Ð½Ñ‹Ðµ)
    output_store = TableStoreDB(
        dbconn,
        "posts_with_username",
        [
            Column("id", String, primary_key=True),
            Column("user_id", String),  # ÐžÐ±Ñ‹Ñ‡Ð½Ð°Ñ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ°, Ð½Ðµ primary key
            Column("content", String),
            Column("username", String),
        ],
        create_table=True,
    )
    output_dt = ds.create_table("posts_with_username", output_store)

    # 4. Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    process_ts = time.time()

    # 3 Ð¿Ð¾ÑÑ‚Ð° Ð¾Ñ‚ 2 Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹
    posts_df = pd.DataFrame([
        {"id": "1", "user_id": "1", "content": "Post 1"},
        {"id": "2", "user_id": "1", "content": "Post 2"},
        {"id": "3", "user_id": "2", "content": "Post 3"},
    ])
    posts.store_chunk(posts_df, now=process_ts)

    # 2 Ð¿Ñ€Ð¾Ñ„Ð¸Ð»Ñ
    profiles_df = pd.DataFrame([
        {"id": "1", "username": "alice"},
        {"id": "2", "username": "bob"},
    ])
    profiles.store_chunk(profiles_df, now=process_ts)

    # 5. Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ñ join_keys
    def transform_func(posts_df, profiles_df):
        # JOIN posts + profiles
        result = posts_df.merge(profiles_df, left_on="user_id", right_on="id", suffixes=("", "_profile"))
        return result[["id", "user_id", "content", "username"]]

    step = BatchTransformStep(
        ds=ds,
        name="test_transform",
        func=transform_func,
        input_dts=[
            ComputeInput(dt=posts, join_type="full"),  # Ð“Ð»Ð°Ð²Ð½Ð°Ñ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð°
            ComputeInput(dt=profiles, join_type="inner", join_keys={"user_id": "id"}),  # JoinSpec Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð°
        ],
        output_dts=[output_dt],
        transform_keys=["id"],  # Primary key Ð¿ÐµÑ€Ð²Ð¾Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ (posts)
        use_offset_optimization=True,  # Ð’ÐÐ–ÐÐž: Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ offset optimization
    )

    # 6. Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ
    print("\nðŸš€ Running initial transformation...")
    step.run_full(ds)

    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸
    output_data = output_dt.get_data()
    print(f"âœ… Output rows created: {len(output_data)}")
    print(f"Output data:\n{output_data}")

    # 7. ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ‡Ñ‚Ð¾ offset'Ñ‹ ÑÐ¾Ð·Ð´Ð°Ð½Ñ‹ Ð´Ð»Ñ ÐžÐ‘Ð•Ð˜Ð¥ Ñ‚Ð°Ð±Ð»Ð¸Ñ†
    print("\nðŸ” Checking offsets...")
    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ step.get_name() Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¸Ð¼Ñ Ñ Ñ…ÑÑˆÐµÐ¼
    transform_name = step.get_name()
    print(f"ðŸ”‘ Transform name with hash: {transform_name}")
    offsets = ds.offset_table.get_offsets_for_transformation(transform_name)

    print(f"ðŸ“Š Offsets created: {offsets}")

    # ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž: offset Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð´Ð»Ñ posts Ð˜ Ð´Ð»Ñ profiles!
    assert "posts" in offsets, "Offset for 'posts' table not found!"
    assert "profiles" in offsets, "Offset for 'profiles' table not found! (Ð‘ÐÐ“!)"

    # ÐžÐ±Ð° offset'Ð° Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ >= process_ts
    assert offsets["posts"] >= process_ts, f"posts offset {offsets['posts']} < process_ts {process_ts}"
    assert offsets["profiles"] >= process_ts, f"profiles offset {offsets['profiles']} < process_ts {process_ts}"

    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‡Ñ‚Ð¾ Ð±Ñ‹Ð»Ð¸ ÑÐ¾Ð·Ð´Ð°Ð½Ñ‹ 3 Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð² output
    output_data = output_dt.get_data()
    assert len(output_data) == 3, f"Expected 3 output rows, got {len(output_data)}"

    # 8. Ð”Ð¾Ð±Ð°Ð²Ð¸Ð¼ Ð½Ð¾Ð²Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ð¼ Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ
    time.sleep(0.01)  # ÐÐµÐ±Ð¾Ð»ÑŒÑˆÐ°Ñ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ° Ð´Ð»Ñ Ñ€Ð°Ð·Ð»Ð¸Ñ‡ÐµÐ½Ð¸Ñ timestamp'Ð¾Ð²
    process_ts2 = time.time()

    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ 1 Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾ÑÑ‚
    new_posts_df = pd.DataFrame([
        {"id": "4", "user_id": "1", "content": "New Post 4"},
    ])
    posts.store_chunk(new_posts_df, now=process_ts2)

    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ 1 Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ„Ð¸Ð»ÑŒ
    new_profiles_df = pd.DataFrame([
        {"id": "3", "username": "charlie"},
    ])
    profiles.store_chunk(new_profiles_df, now=process_ts2)

    # 9. Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ
    step.run_full(ds)

    # 10. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‡Ñ‚Ð¾ offset'Ñ‹ Ð¾Ð±Ð½Ð¾Ð²Ð¸Ð»Ð¸ÑÑŒ
    new_offsets = ds.offset_table.get_offsets_for_transformation(transform_name)

    print(f"\nðŸ“Š New offsets after incremental run: {new_offsets}")

    # ÐžÐ±Ð° offset'Ð° Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒÑÑ Ð´Ð¾ process_ts2
    assert new_offsets["posts"] >= process_ts2, f"posts offset not updated: {new_offsets['posts']} < {process_ts2}"
    assert new_offsets["profiles"] >= process_ts2, f"profiles offset not updated: {new_offsets['profiles']} < {process_ts2}"

    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‡Ñ‚Ð¾ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ 4 Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð² output (3 ÑÑ‚Ð°Ñ€Ñ‹Ñ… + 1 Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾ÑÑ‚)
    output_data = output_dt.get_data()
    assert len(output_data) == 4, f"Expected 4 output rows, got {len(output_data)}"

    print("\nâœ… SUCCESS: Offsets created and updated for both posts AND profiles (including JoinSpec table)!")


def test_joinspec_update_ts_from_meta_table_not_null(dbconn: DBConn):
    """
    ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð°: Ð”Ð»Ñ join_keys (reverse join) update_ts Ð¿Ð¾Ð´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ÑÑ ÐºÐ°Ðº NULL
    Ð¸Ð· primary_data_tbl, Ñ‡Ñ‚Ð¾ Ð¿Ñ€Ð¸Ð²Ð¾Ð´Ð¸Ñ‚ Ðº Ð¿ÐµÑ€ÐµÐ¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Ð½Ð° ÐºÐ°Ð¶Ð´Ð¾Ð¼ Ð·Ð°Ð¿ÑƒÑÐºÐµ.

    Ð¡Ñ†ÐµÐ½Ð°Ñ€Ð¸Ð¹:
    1. Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð¾ÑÐ½Ð¾Ð²Ð½ÑƒÑŽ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ (posts) Ð¸ ÑÐ¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½ÑƒÑŽ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ (profiles)
    2. Ð¡Ð²ÑÐ·Ð°Ñ‚ÑŒ Ñ‡ÐµÑ€ÐµÐ· join_keys
    3. Ð˜Ð·Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ ÑÐ¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½ÑƒÑŽ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ
    4. ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ñ‡ÐµÑ€ÐµÐ· reverse join
    5. ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€Ð½Ð¾ Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ - Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ° Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ ÐŸÐ£Ð¡Ð¢ÐžÐ™ (Ð½ÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ñ… Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹)

    ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ: update_ts Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ€Ð°Ñ‚ÑŒÑÑ Ð¸Ð· Ð¼ÐµÑ‚Ð°-Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ ÑÐ¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½Ð¸ÐºÐ° (tbl.c.update_ts),
    Ð° Ð½Ðµ Ð¿Ð¾Ð´ÑÑ‚Ð°Ð²Ð»ÑÑ‚ÑŒÑÑ ÐºÐ°Ðº NULL Ð¸Ð· primary_data_tbl

    ÐšÐ¾Ð´ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð² sql_meta.py (ÑÑ‚Ñ€Ð¾ÐºÐ¸ 930-936):
    - update_ts Ð±ÐµÑ€ÐµÑ‚ÑÑ Ð¸Ð· tbl.c.update_ts Ð²Ð¼ÐµÑÑ‚Ð¾ NULL
    - update_ts Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ Ð² GROUP BY
    """
    ds = DataStore(dbconn, create_meta_table=True)

    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¾ÑÐ½Ð¾Ð²Ð½ÑƒÑŽ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ (posts)
    posts_store = TableStoreDB(
        dbconn,
        "posts_table",
        [
            Column("post_id", String, primary_key=True),
            Column("user_id", String),
            Column("content", String),
        ],
        create_table=True,
    )
    posts_dt = ds.create_table("posts_table", posts_store)

    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑÐ¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½ÑƒÑŽ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ (profiles)
    profiles_store = TableStoreDB(
        dbconn,
        "profiles_table",
        [
            Column("id", String, primary_key=True),
            Column("name", String),
        ],
        create_table=True,
    )
    profiles_dt = ds.create_table("profiles_table", profiles_store)

    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð²Ñ‹Ñ…Ð¾Ð´Ð½ÑƒÑŽ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ
    output_store = TableStoreDB(
        dbconn,
        "enriched_posts",
        [
            Column("post_id", String, primary_key=True),
            Column("user_id", String),
            Column("content", String),
        ],
        create_table=True,
    )
    output_dt = ds.create_table("enriched_posts", output_store)

    def join_func(posts_df, profiles_df):
        # ÐžÐ±Ð¾Ð³Ð°Ñ‰Ð°ÐµÐ¼ Ð¿Ð¾ÑÑ‚Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ð¸Ð· profiles (Ñ…Ð¾Ñ‚Ñ Ð² ÑÑ‚Ð¾Ð¼ Ñ‚ÐµÑÑ‚Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ posts)
        return posts_df[["post_id", "user_id", "content"]]

    step = BatchTransformStep(
        ds=ds,
        name="join_test",
        func=join_func,
        input_dts=[
            ComputeInput(dt=posts_dt, join_type="full"),  # ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð°
            ComputeInput(
                dt=profiles_dt,
                join_type="full",
                join_keys={"user_id": "id"},  # Reverse join
            ),
        ],
        output_dts=[output_dt],
        transform_keys=["post_id"],
        use_offset_optimization=True,
        chunk_size=10,
    )

    # 1. Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ðµ
    t1 = time.time()
    posts_df = pd.DataFrame({
        "post_id": ["post_1", "post_2"],
        "user_id": ["user_1", "user_2"],
        "content": ["Hello", "World"],
    })
    posts_dt.store_chunk(posts_df, now=t1)

    # 2. Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² ÑÐ¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½Ð¾Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ðµ
    time.sleep(0.01)
    t2 = time.time()
    profiles_df = pd.DataFrame({
        "id": ["user_1", "user_2"],
        "name": ["Alice", "Bob"],
    })
    profiles_dt.store_chunk(profiles_df, now=t2)

    # 3. ÐŸÐµÑ€Ð²Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð³Ð¾Ð½ - Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð²ÑÐµ Ð·Ð°Ð¿Ð¸ÑÐ¸
    step.run_full(ds)

    output_data = output_dt.get_data()
    assert len(output_data) == 2
    assert set(output_data["post_id"]) == {"post_1", "post_2"}

    # 4. ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ offset
    offsets = ds.offset_table.get_offsets_for_transformation(step.get_name())
    assert "posts_table" in offsets
    assert "profiles_table" in offsets
    posts_offset = offsets["posts_table"]
    profiles_offset = offsets["profiles_table"]

    # 5. ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð³Ð¾Ð½ - Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ° Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ ÐŸÐ£Ð¡Ð¢ÐžÐ™ (Ð½ÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ñ… Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹)
    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð±Ð°Ñ‚Ñ‡ÐµÐ¹ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸
    idx_count, idx_gen = step.get_full_process_ids(ds=ds, run_config=None)

    assert idx_count == 0, (
        f"ÐŸÐ¾ÑÐ»Ðµ Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾Ð³Ð¾ run_full Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ° Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ Ð¿ÑƒÑÑ‚Ð¾Ð¹. "
        f"ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ {idx_count} Ð±Ð°Ñ‚Ñ‡ÐµÐ¹ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸. "
        f"Ð­Ñ‚Ð¾ ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð½Ð° Ñ‚Ð¾, Ñ‡Ñ‚Ð¾ offset-Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ JoinSpec ÐÐ• Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚: "
        f"Ð·Ð°Ð¿Ð¸ÑÐ¸ Ñ join_keys Ð¿ÐµÑ€ÐµÐ¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽÑ‚ÑÑ Ð½Ð° ÐºÐ°Ð¶Ð´Ð¾Ð¼ Ð·Ð°Ð¿ÑƒÑÐºÐµ Ð¸Ð·-Ð·Ð° update_ts = NULL."
    )

    print("\nâœ… SUCCESS: JoinSpec update_ts correctly taken from meta table, no reprocessing!")
